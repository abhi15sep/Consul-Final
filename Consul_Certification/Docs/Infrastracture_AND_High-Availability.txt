Implementing High-Availability in Consul:
=========================================
Understanding the Challenge:
As of now, we have been making use of a single server and a client for our learning.
If the server goes down, then all of your data will be inaccessible and requests would fail.

Ideal Setup:
In a production environment, you should have multiple sets of servers for high-availability.

Overview of Bootstrap Expect:
Bootstrap Expect flag informs Consul of the expected number of server nodes and automatically bootstraps when that many servers are available.
If you have 3 servers that will be part of the cluster, bootstrap-expect should be 3.

Examples:
Server 1:
consul agent -server=true -client=0.0.0.0 -bind [NODE-IP] -bootstrap-expect 3 -data-dir /tmp/consul

Server 2:
consul agent -server=true -client=0.0.0.0 -bind [NODE-IP]  -data-dir /tmp/consul
consul join [FIRST-SERVER-IP]

Server 3:
consul agent -server=true -client=0.0.0.0 -bind [NODE-IP]  -data-dir /tmp/consul
consul join [FIRST-SERVER-IP]

Verification:
consul members

API Endpoint:
http://SERVER-IP:8500/v1/status/leader

Multiple Datacenter in Consul:
==============================
Revising Concept of Datacenter:
A datacenter is a networking environment that is private, low latency, and high bandwidth.
This excludes communication that would traverse the public internet.
Multiple Availability Zone within a single AWS region would be considered part of a single datacenter.

Multiple Datacenter approach:
A large scale organization can be hosting their infrastructure across multiple cloud platforms
and even on-premise.

Important Pointers:
All server nodes must be able to talk to each other. Otherwise, the gossip protocol as well as RPC forwarding will not work.
In general, data is not replicated between different Consul datacenters. When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results.

Example:

Server 1:
data_dir =  "/etc/consul.d/consul-dir"
bind_addr = "134.209.155.89"
client_addr = "0.0.0.0"
bootstrap_expect = 1
node_name = "consul-server"
ui = true
server = true
datacenter = "India"

Server 2:
data_dir =  "/etc/consul.d/consul-dir"
bind_addr = "165.22.222.190"
client_addr = "0.0.0.0"
bootstrap_expect = 1
node_name = "consul-server-2"
ui = true
server = true
datacenter = "Singapore"

Join WAN:
consul members -wan
consul join -wan [SERVER-1-IP]
consul catalog datacenters

Verification:
curl  http://127.0.0.1:8500/v1/kv/dc
curl  http://127.0.0.1:8500/v1/kv/dc?dc=singapore

echo U2luZ2Fwb3Jl | base64 -d

Overview of Prepared Query:
============================ 
Understanding Prepared Query:
Prepared queries are objects that are defined at the datacenter level.
Once created, prepared queries can then be invoked by applications to perform the query and get the latest results.

Use Case 1 - Multiple Versions:
Letâ€™s assume there are multiple versions of AppA: v1 and v2
Within the prepared query, we can explicitly specify a version that needs to be returned to the client.
There are no configuration changes required at the client-side.

Use Case 2 - Failover Policy:
You can contact other data centers once there are no healthy instances in the local datacenter.

Steps Involved in Failover Policy:
Consul servers in the local datacenter will attempt to find healthy instances of the "AppA" service within the dc1
If none are available locally, the Consul servers will make an RPC request to the Consul servers in "dc2" to perform the query there.
Finally, an error will be returned if none of these datacenters had any instances available.

> cat prepared-query.json
{
  "Name": "web-service",
  "Service": {
    "Service": "web",
    "Tags": ["v2"]
  }
}

> dig @localhost -p 8600 v1.web.service.consul SRV
> dig @localhost -p 8600 v2.web.service.consul SRV
> dig @localhost -p 8600 web-service.query.consul SRV (prepared query)


> nano failover.json
{
  "Name": "failover",
  "Service": {
    "Service": "database",
    "Tags": ["v1"],
    "Failover": {
      "Datacenters": ["singapore"]
    }
  }
}
> dig @localhost -p 8600 failover.query.consul SRV

Examples (prepared-query-use-case-1):

Documentation Referred:
https://www.consul.io/api-docs/query

Step 1: Create two services
cd /etc/consul.d

Service 1:
nano web-1.json
{
  "service": {
    "name": "web",
     "id": "web1",
    "port": 8080,
    "tags": ["v1"]
  }
}

Service 2:
nano web-2.json
{
  "service": {
    "name": "web",
    "id":   "web2",
    "port": 9080,
    "tags": ["v2"]
  }
}

consul reload

Use-Case 1: Creating a basic prepared query:
cd /tmp
nano prepared-query.json

{
  "Name": "web-service",
  "Service": {
    "Service": "web",
    "Tags": ["v2"]
  }
  
curl --request POST --data @prepared-query.json http://127.0.0.1:8500/v1/query
dig @localhost -p 8600 web-service.query.consul SRV

List Prepared Query:
curl http://127.0.0.1:8500/v1/query

Update Prepared Query:
curl --request PUT --data @prepared-query.json http://127.0.0.1:8500/v1/query/4de2cfbc-7c58-c4d4-8b39-ba39a7f765c8

Deleted Prepared Query:
curl --request DELETE http://127.0.0.1:8500/v1/query/:uuid


Examples (Use-Case 2 Static Failover Policy):
Singapore DC:

cd /etc/consul.d
nano db.json

{
  "service": {
    "name": "database",
    "port": 5080,
    "tags": ["v1"]
  }
}

consul reload

India DC:

cd /tmp
nano failover.json

{
  "Name": "failover",
  "Service": {
    "Service": "database",
    "Tags": ["v1"],
    "Failover": {
      "Datacenters": ["singapore"]
    }
  }
}

curl --request POST --data @failover.json http://127.0.0.1:8500/v1/query

Verification:
India DC:

dig @localhost -p 8600 failover.query.consul SRV

NearestN in prepared query specifies that the query will be forwarded to up to NearestN other datacenters based on their estimated network round trip time using Network Coordinates from the WAN gossip pool.

Example Snippet:

      "Service": {
        "Service": "web",
        "Failover": {
          "NearestN": 3
        }
      }

This will attempt to find a service locally, and otherwise, attempt to find that service in the next three closest datacenters.

Backup & Restore:
=================
All servers write to the -data-dir before commit on write requests
Consul provides the snapshot command that saves a point-in-time snapshot of the state of the Consul servers. 
Some of the important data includes:
a) Key-Value entries
b) the service catalog
c) prepared queries
d) sessions
e) ACLs

Following commands can be used to take backup and restore data from backup.
commands                               Description
consul snapshot save backup.snap       Takes backup and stores it to backup.snap file
consul snapshot restore backup.snap    Restore data from the backup

Important Note:
Snapshots will not be saved if the datacenter is degraded or if no leader is available.
It is possible to run the snapshot on any non-leader server using stale consistency mode. This means that a very small number of recent writes may not be included.
It is recommended to regularly take a backup in an automated way.

Pre-Requisite:
We had stored this data via GUI in demo. This is an equivalent command.
consul kv put name backup1

Step 1: Take a backup
consul snapshot save backup.snap
consul kv delete name

Step 2: Restore From Backup
consul snapshot restore backup.snap
consul kv get name

Overview of AutoPilot:
======================
Overview of AutoPilot Pattern:
The autopilot pattern automates in code the repetitive and boring operational tasks of an application, including startup, shutdown, scaling, and recovery from anticipated failure conditions for reliability, ease of use, and improved productivity.

Use-Case 1 - Dead Servers:
It will take 72 hours for dead servers to be automatically reaped or an operator must write a script to consul force-leave.
Autopilot helps prevent these kinds of outages by quickly removing failed servers as soon as a replacement Consul server comes online.

Use-Case 2 - Server Stabilization Time:
When a new server is added to the data center, there is a waiting period where it must be healthy and stable for a certain amount of time before being promoted to a full, voting member.
This is defined by the ServerStabilizationTime autopilot's parameter and by default is 10 seconds.
In case your configuration requires a different amount of time for the node to get ready, you can tune the parameter and assign it to a different duration.

Additional Features for Enterprise:
There are some additional features of autopilot that are available in Consul Enterprise
a) Redundancy Zones
b) Automated upgrades

Example:
First Server (consul-01):
consul agent -server -bootstrap-expect=3 -node=consul-server -bind=134.209.155.89 -data-dir=/tmp/consul -client=0.0.0.0 -ui=true

Second Server (consul-02):
consul agent -server=true -client=0.0.0.0 -bind 165.22.222.190 -data-dir /tmp/consul
consul join 134.209.155.89

Third Server (demo-02):
consul agent -server=true -client=0.0.0.0 -bind 165.22.222.190 -data-dir /tmp/consul
consul join 134.209.155.89

See Details of Servers & Verify Autopilot settings (consul-01):
consul operator raft list-peers
consul operator autopilot get-config

Modify Autopilot Settings (leader node):
consul operator autopilot set-config -server-stabilization-time=60s

Leave and Join consul-02 node:
consul leave
consul agent -server=true -client=0.0.0.0 -bind [NODE-IP]  -data-dir /tmp/consul
consul join [FIRST-SERVER-IP]

Monitor the data to see how much time it takes for it to be added as Voter (consul-01):
consul operator raft list-peers

Automatically joining servers:
==============================

Approaches to join servers:
There are multiple options for joining the servers:
a) Specify a list of servers with -join and start_join options.
b) Specify a list of servers with -retry-join option
c) Use automatic joining by tag for supported cloud environments with the -retry-join

You can choose the method which best suits your environment and specific use-case.

Approach1: 
> consul join server-node
If server is not available, this command will fail and you will see that your cluster is not created.

Approach2: Retry Join:
Retry join is similar to -join but allows retrying a join until its successful.
> consul agent -retry-join "10.0.4.67"
This is useful for cases where you know the address will eventually be available.
retry_join could be more appropriate to help mitigate node startup race conditions when automating a consul cluster deployment.

Example:
Pre-Requisite:
Make sure that consul is stopped via systemd Data-Dir path should be empty in the client.

Server:
consul agent -dev -client=0.0.0.0 -bind 159.65.145.160

Client:
consul agent -retry-join 159.65.145.160 -bind  134.209.154.246 -data-dir /root/consul -retry-interval 5s